{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer 검증 프로그램\n",
    "- Numpy, module 활용 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimizer as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수 및 도함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x,y) = x*x + y*y + xy - 4x - 8y\n",
    "def func(params):\n",
    "    x, y = params[0], params[1]\n",
    "    return x*x + y*y + x*y - 4.*x - 8.*y\n",
    "\n",
    "# Df(x,y) = (2x + y - 4, 2y + x - 8)\n",
    "def deriv_f(params):\n",
    "    x, y = params[0], params[1]\n",
    "    return np.array((np.round(2*x + y - 4., 4), np.round(2*y + x - 8., 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD(Stochastic Gradient Descent): 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[2. 4.], grads=[-4. -8.], func=-12.0000\n",
      "2회 시행: params=[0. 3.], grads=[4. 2.], func=-15.0000\n",
      "3회 시행: params=[0.5 4. ], grads=[-1. -2.], func=-15.7500\n",
      "4회 시행: params=[0.   3.75], grads=[1.  0.5], func=-15.9375\n",
      "5회 시행: params=[0.125 4.   ], grads=[-0.25 -0.5 ], func=-15.9844\n",
      "6회 시행: params=[0.     3.9375], grads=[0.25  0.125], func=-15.9961\n",
      "7회 시행: params=[0.03125 4.     ], grads=[-0.0625 -0.125 ], func=-15.9990\n",
      "8회 시행: params=[0.     3.9844], grads=[0.0625 0.0312], func=-15.9998\n",
      "9회 시행: params=[0.0078 4.    ], grads=[-0.0156 -0.0312], func=-15.9999\n",
      "10회 시행: params=[-2.1159649e-10  3.9960999e+00], grads=[0.0156 0.0078], func=-16.0000\n"
     ]
    }
   ],
   "source": [
    "sgd = opt.SGD(0.5)\n",
    "params = np.array((0,0), dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    sgd.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum\n",
    "    - Gradient Descent에 현재의 관성을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[2. 4.], grads=[-4. -8.], v=[2. 4.], func=-12.0000\n",
      "2회 시행: params=[1. 5.], grads=[4. 2.], v=[-1.  1.], func=-13.0000\n",
      "3회 시행: params=[-1.  4.], grads=[3. 3.], v=[-2. -1.], func=-15.0000\n",
      "4회 시행: params=[-1.  4.], grads=[-2. -1.], v=[0. 0.], func=-15.0000\n",
      "5회 시행: params=[0.  4.5], grads=[-2. -1.], v=[1.  0.5], func=-15.7500\n",
      "6회 시행: params=[0.25 4.25], grads=[0.5 1. ], v=[ 0.25 -0.25], func=-15.8125\n",
      "7회 시행: params=[0.   3.75], grads=[0.75 0.75], v=[-0.25 -0.5 ], func=-15.9375\n",
      "8회 시행: params=[0.   3.75], grads=[-0.25 -0.5 ], v=[0. 0.], func=-15.9375\n",
      "9회 시행: params=[0.125 4.   ], grads=[-0.25 -0.5 ], v=[0.125 0.25 ], func=-15.9844\n",
      "10회 시행: params=[0.0625 4.0625], grads=[0.25  0.125], v=[-0.0625  0.0625], func=-15.9883\n"
     ]
    }
   ],
   "source": [
    "momentum = opt.Momentum(lr=0.5, momentum=0.5)\n",
    "params = np.zeros(2, dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    momentum.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, v={momentum.v}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NAG(Nesterov Accelerated Gradient)\n",
    "    - 현재 위치에서의 관성과 관성방향으로 움직인 후 위치에서의 gradient 반대방향을 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[1.44 2.88], grads=[-4. -8.], v=[-0.0625  0.0625], func=-14.2848\n",
      "2회 시행: params=[1.3184 4.192 ], grads=[ 1.76 -0.8 ], v=[-0.0625  0.0625], func=-13.9718\n",
      "3회 시행: params=[0.484352  4.5007358], grads=[2.8288 1.7024], v=[-0.0625  0.0625], func=-15.2721\n",
      "4회 시행: params=[-0.2592624  4.485221 ], grads=[1.4694 1.4858], v=[-0.0625  0.0625], func=-15.8231\n",
      "5회 시행: params=[-0.6070619  4.4545045], grads=[-0.0333  0.7112], v=[-0.0625  0.0625], func=-15.7008\n",
      "6회 시행: params=[-0.61717355  4.4350395 ], grads=[-0.7596  0.3019], v=[-0.0625  0.0625], func=-15.6983\n",
      "7회 시행: params=[-0.4590508  4.3767276], grads=[-0.7993  0.2529], v=[-0.0625  0.0625], func=-15.8203\n",
      "8회 시행: params=[-0.26553664  4.264558  ], grads=[-0.5414  0.2944], v=[-0.0625  0.0625], func=-15.9298\n",
      "9회 시행: params=[-0.10140929  4.1270304 ], grads=[-0.2665  0.2636], v=[-0.0625  0.0625], func=-15.9865\n",
      "10회 시행: params=[0.01454058 4.004212  ], grads=[-0.0758  0.1527], v=[-0.0625  0.0625], func=-15.9997\n"
     ]
    }
   ],
   "source": [
    "nag = opt.NAG(lr=0.2, momentum=0.8)\n",
    "params = np.zeros(2, dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    nag.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, v={momentum.v}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaGrad\n",
    "    - 일정한 learning rate를 사용하지 않고 변수마다 그리고 스텝마다 learning rate가 바뀜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[10. 10.], grads=[-4. -8.], h=[16. 64.], func=180.0000\n",
      "2회 시행: params=[0.1163 0.6021], grads=[26. 22.], h=[692. 548.], func=-4.8359\n",
      "3회 시행: params=[1.3109 3.3459], grads=[-3.1653 -6.6795], h=[702.0191 592.6157], func=-14.7112\n",
      "4회 시행: params=[0.5703 3.3448], grads=[1.9677 0.0027], h=[705.8909 592.6157], func=-15.6191\n",
      "5회 시행: params=[0.3876 3.6487], grads=[ 0.4854 -0.7401], h=[706.1265 593.1635], func=-15.8625\n",
      "6회 시행: params=[0.2281 3.778 ], grads=[ 0.4239 -0.315 ], h=[706.3062 593.2627], func=-15.9493\n",
      "7회 시행: params=[0.14   3.8666], grads=[ 0.2342 -0.2159], h=[706.3611 593.3093], func=-15.9813\n",
      "8회 시행: params=[0.0848 3.9187], grads=[ 0.1466 -0.1268], h=[706.3826 593.3254], func=-15.9931\n",
      "9회 시행: params=[0.0516 3.9506], grads=[ 0.0883 -0.0778], h=[706.3904 593.3314], func=-15.9974\n",
      "10회 시행: params=[0.0314 3.97  ], grads=[ 0.0538 -0.0472], h=[706.3933 593.3336], func=-15.9991\n"
     ]
    }
   ],
   "source": [
    "adg = opt.AdaGrad(lr=10)\n",
    "params = np.zeros(2, dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    adg.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, h={adg.h}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSProp\n",
    "    - AdaGrad는 스텝이 많이 진행되면 h 값이 너무 커져서 학습률이 너무 작아져 학습이 거의 되지 않음\n",
    "    - 이를 보완하기 위해 이전 누적치와 현재 그래디언트의 좌표별 제곱의 가중치 평균을 반영함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[1.8 1.8], grads=[-4. -8.], h=[ 4. 16.], func=-11.8800\n",
      "2회 시행: params=[1.1255 2.4324], grads=[ 1.4 -2.6], h=[ 3.49 13.69], func=-14.0402\n",
      "3회 시행: params=[0.7535 2.971 ], grads=[ 0.6834 -2.0097], h=[ 2.7343 11.2772], func=-15.1487\n",
      "4회 시행: params=[0.4572 3.3649], grads=[ 0.478  -1.3045], h=[2.1078 8.8833], func=-15.6780\n",
      "5회 시행: params=[0.2585 3.6449], grads=[ 0.2793 -0.813 ], h=[1.6004 6.8277], func=-15.8989\n",
      "6회 시행: params=[0.1259 3.8237], grads=[ 0.1619 -0.4517], h=[1.2069 5.1718], func=-15.9753\n",
      "7회 시행: params=[0.0545 3.9271], grads=[ 0.0755 -0.2267], h=[0.9066 3.8917], func=-15.9957\n",
      "8회 시행: params=[0.0151 3.9752], grads=[ 0.0361 -0.0913], h=[0.6803 2.9209], func=-15.9995\n",
      "9회 시행: params=[0.0083 3.9962], grads=[ 0.0054 -0.0345], h=[0.5102 2.191 ], func=-15.9999\n",
      "10회 시행: params=[-0.0103  3.9957], grads=[0.0128 0.0007], h=[0.3827 1.6433], func=-15.9998\n"
     ]
    }
   ],
   "source": [
    "rmsp = opt.RMSProp(lr=0.9, gamma=0.75)\n",
    "params = np.zeros(2, dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    rmsp.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, h={rmsp.h}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam\n",
    "    - Momentum과 RMSProp 두가지 방식을 혼합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기값: params=[0. 0.], grads=[-4. -8.], func=0.0000\n",
      "1회 시행: params=[0.9 0.9], grads=[-4. -8.], m=[-0.4 -0.8], v=[0.016 0.064], func=-8.3700\n",
      "2회 시행: params=[1.68   1.7728], grads=[-1.3 -5.3], m=[-0.49 -1.25], v=[0.0177 0.092 ], func=-11.9589\n",
      "3회 시행: params=[2.1122 2.5807], grads=[ 1.1328 -2.7744], m=[-0.3277 -1.4024], v=[0.019  0.0996], func=-12.5220\n",
      "4회 시행: params=[2.1267 3.2788], grads=[ 2.8051 -0.7264], m=[-0.0144 -1.3348], v=[0.0268 0.1   ], func=-12.4908\n",
      "5회 시행: params=[1.8599 3.8339], grads=[3.5322 0.6843], m=[ 0.3403 -1.1329], v=[0.0392 0.1004], func=-12.8221\n",
      "6회 시행: params=[1.4279 4.236 ], grads=[3.5537 1.5277], m=[ 0.6616 -0.8668], v=[0.0518 0.1026], func=-13.5684\n",
      "7회 시행: params=[0.9013 4.4971], grads=[3.0918 1.8999], m=[ 0.9046 -0.5901], v=[0.0613 0.1061], func=-14.4925\n",
      "8회 시행: params=[0.33   4.6426], grads=[2.2997 1.8955], m=[ 1.0441 -0.3415], v=[0.0665 0.1096], func=-15.2661\n",
      "9회 시행: params=[-0.2404  4.7032], grads=[1.3026 1.6152], m=[ 1.07   -0.1458], v=[0.0681 0.1121], func=-15.6168\n",
      "10회 시행: params=[-0.7609  4.7092], grads=[0.2224 1.166 ], m=[ 0.9852 -0.0146], v=[0.0681 0.1133], func=-15.4577\n"
     ]
    }
   ],
   "source": [
    "adam = opt.Adam(lr=0.9)\n",
    "params = np.zeros(2, dtype=np.float32)\n",
    "grads = deriv_f(params)\n",
    "print(f'초기값: params={params}, grads={grads}, func={func(params):.4f}')\n",
    "for i in range(10):\n",
    "    adam.update(params, grads)\n",
    "    print(f'{i+1}회 시행: params={params}, grads={grads}, m={adam.m}, v={adam.v}, func={func(params):.4f}')\n",
    "    grads = deriv_f(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a1baeb1610b05443f415525bf52a486212d0ee94c2d320214bf0d7d56e225dd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('vsc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
