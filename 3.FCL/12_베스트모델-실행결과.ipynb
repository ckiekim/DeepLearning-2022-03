{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_베스트모델.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 12. 베스트 모델 저장하기\n",
        "### - 이진 분류 - 유방암 예측 사례"
      ],
      "metadata": {
        "id": "Z-X-A2DWlwiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TD5BpXhZmLij"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 준비"
      ],
      "metadata": {
        "id": "-zlzcx5toBTi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BN5NDMhzlfrM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "cancer_std = scaler.fit_transform(cancer.data)"
      ],
      "metadata": {
        "id": "_hRk4U_jpSwg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer_std, cancer.target, stratify=cancer.target, test_size=0.2, random_state=2022\n",
        ")\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "Dyn3-HGznGqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c7263b-d578-4ada-ceea-ddde3d21dcc0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((455, 30), (114, 30), (455,), (114,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Best Model 저장하고 불러오기"
      ],
      "metadata": {
        "id": "YX4xCv0mHU75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 정의"
      ],
      "metadata": {
        "id": "0q2mIoPYoHvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "_8Amy7D5np4X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(100, input_shape=(30,), activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "C-kdkjHfusXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd9e4e3-36e4-4b0c-8383-796a4600ab8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               3100      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                2424      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,549\n",
            "Trainable params: 5,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 설정"
      ],
      "metadata": {
        "id": "evEWBAcDv6Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Cy5iuRbcv1QB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Callback 설정 - 베스트 모델 저장"
      ],
      "metadata": {
        "id": "Sa_yOo92FOW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "modelpath = 'best_model.h5'\n",
        "mc = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1,\n",
        "                     save_best_only=True)"
      ],
      "metadata": {
        "id": "mh_PCwc2FNUz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 학습"
      ],
      "metadata": {
        "id": "jBOtOphMzllW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=50,\n",
        "                    verbose=1, callbacks=[mc])"
      ],
      "metadata": {
        "id": "ITEwb5V8zj9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff263642-1f46-4f99-c8ed-6ec4a8cbc648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/8 [==>...........................] - ETA: 5s - loss: 0.8323 - accuracy: 0.4400\n",
            "Epoch 1: val_loss improved from inf to 0.48854, saving model to best_model.h5\n",
            "8/8 [==============================] - 1s 34ms/step - loss: 0.7180 - accuracy: 0.4973 - val_loss: 0.4885 - val_accuracy: 0.8571\n",
            "Epoch 2/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.5116 - accuracy: 0.8200\n",
            "Epoch 2: val_loss improved from 0.48854 to 0.32092, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.9038 - val_loss: 0.3209 - val_accuracy: 0.9231\n",
            "Epoch 3/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3101 - accuracy: 0.9000\n",
            "Epoch 3: val_loss improved from 0.32092 to 0.23685, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2777 - accuracy: 0.9313 - val_loss: 0.2369 - val_accuracy: 0.9231\n",
            "Epoch 4/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1995 - accuracy: 0.9600\n",
            "Epoch 4: val_loss improved from 0.23685 to 0.19001, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2058 - accuracy: 0.9396 - val_loss: 0.1900 - val_accuracy: 0.9451\n",
            "Epoch 5/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1351 - accuracy: 0.9800\n",
            "Epoch 5: val_loss improved from 0.19001 to 0.16298, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1663 - accuracy: 0.9451 - val_loss: 0.1630 - val_accuracy: 0.9451\n",
            "Epoch 6/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1624 - accuracy: 0.9400\n",
            "Epoch 6: val_loss improved from 0.16298 to 0.14771, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1385 - accuracy: 0.9533 - val_loss: 0.1477 - val_accuracy: 0.9560\n",
            "Epoch 7/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1717 - accuracy: 0.9200\n",
            "Epoch 7: val_loss improved from 0.14771 to 0.13863, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1198 - accuracy: 0.9588 - val_loss: 0.1386 - val_accuracy: 0.9560\n",
            "Epoch 8/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9400\n",
            "Epoch 8: val_loss improved from 0.13863 to 0.13332, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1056 - accuracy: 0.9670 - val_loss: 0.1333 - val_accuracy: 0.9560\n",
            "Epoch 9/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0981 - accuracy: 0.9800\n",
            "Epoch 9: val_loss improved from 0.13332 to 0.12893, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9808 - val_loss: 0.1289 - val_accuracy: 0.9560\n",
            "Epoch 10/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1158 - accuracy: 0.9800\n",
            "Epoch 10: val_loss improved from 0.12893 to 0.12710, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0866 - accuracy: 0.9835 - val_loss: 0.1271 - val_accuracy: 0.9560\n",
            "Epoch 11/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0492 - accuracy: 1.0000\n",
            "Epoch 11: val_loss improved from 0.12710 to 0.12314, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0799 - accuracy: 0.9808 - val_loss: 0.1231 - val_accuracy: 0.9560\n",
            "Epoch 12/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1532 - accuracy: 0.9600\n",
            "Epoch 12: val_loss improved from 0.12314 to 0.12313, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0742 - accuracy: 0.9835 - val_loss: 0.1231 - val_accuracy: 0.9451\n",
            "Epoch 13/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9800\n",
            "Epoch 13: val_loss improved from 0.12313 to 0.12179, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0693 - accuracy: 0.9808 - val_loss: 0.1218 - val_accuracy: 0.9451\n",
            "Epoch 14/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0497 - accuracy: 1.0000\n",
            "Epoch 14: val_loss improved from 0.12179 to 0.12003, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0650 - accuracy: 0.9835 - val_loss: 0.1200 - val_accuracy: 0.9451\n",
            "Epoch 15/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0801 - accuracy: 0.9800\n",
            "Epoch 15: val_loss improved from 0.12003 to 0.11705, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0611 - accuracy: 0.9863 - val_loss: 0.1171 - val_accuracy: 0.9451\n",
            "Epoch 16/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 16: val_loss improved from 0.11705 to 0.11560, saving model to best_model.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0581 - accuracy: 0.9863 - val_loss: 0.1156 - val_accuracy: 0.9451\n",
            "Epoch 17/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0685 - accuracy: 0.9600\n",
            "Epoch 17: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0549 - accuracy: 0.9890 - val_loss: 0.1181 - val_accuracy: 0.9451\n",
            "Epoch 18/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 18: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0523 - accuracy: 0.9890 - val_loss: 0.1185 - val_accuracy: 0.9451\n",
            "Epoch 19/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0486 - accuracy: 0.9800\n",
            "Epoch 19: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9890 - val_loss: 0.1200 - val_accuracy: 0.9451\n",
            "Epoch 20/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0475 - accuracy: 0.9890 - val_loss: 0.1184 - val_accuracy: 0.9451\n",
            "Epoch 21/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 21: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9890 - val_loss: 0.1180 - val_accuracy: 0.9560\n",
            "Epoch 22/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1003 - accuracy: 0.9800\n",
            "Epoch 22: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0436 - accuracy: 0.9890 - val_loss: 0.1185 - val_accuracy: 0.9451\n",
            "Epoch 23/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0415 - accuracy: 0.9890 - val_loss: 0.1184 - val_accuracy: 0.9560\n",
            "Epoch 24/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0555 - accuracy: 0.9600\n",
            "Epoch 24: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9890 - val_loss: 0.1187 - val_accuracy: 0.9560\n",
            "Epoch 25/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0193 - accuracy: 1.0000\n",
            "Epoch 25: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 0.1217 - val_accuracy: 0.9560\n",
            "Epoch 26/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0362 - accuracy: 0.9890 - val_loss: 0.1217 - val_accuracy: 0.9560\n",
            "Epoch 27/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0345 - accuracy: 0.9890 - val_loss: 0.1216 - val_accuracy: 0.9560\n",
            "Epoch 28/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0269 - accuracy: 0.9800\n",
            "Epoch 28: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 0.1237 - val_accuracy: 0.9560\n",
            "Epoch 29/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0357 - accuracy: 0.9800\n",
            "Epoch 29: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.1250 - val_accuracy: 0.9560\n",
            "Epoch 30/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0297 - accuracy: 0.9918 - val_loss: 0.1273 - val_accuracy: 0.9560\n",
            "Epoch 31/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 31: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 0.1306 - val_accuracy: 0.9560\n",
            "Epoch 32/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0603 - accuracy: 0.9800\n",
            "Epoch 32: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 0.1341 - val_accuracy: 0.9560\n",
            "Epoch 33/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 33: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 0.9945 - val_loss: 0.1367 - val_accuracy: 0.9560\n",
            "Epoch 34/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 0.9945 - val_loss: 0.1397 - val_accuracy: 0.9560\n",
            "Epoch 35/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 0.9945 - val_loss: 0.1420 - val_accuracy: 0.9560\n",
            "Epoch 36/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0210 - accuracy: 0.9945 - val_loss: 0.1426 - val_accuracy: 0.9560\n",
            "Epoch 37/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 0.1446 - val_accuracy: 0.9560\n",
            "Epoch 38/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0248 - accuracy: 0.9800\n",
            "Epoch 38: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0192 - accuracy: 0.9945 - val_loss: 0.1520 - val_accuracy: 0.9560\n",
            "Epoch 39/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0254 - accuracy: 0.9800\n",
            "Epoch 39: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 0.9973 - val_loss: 0.1512 - val_accuracy: 0.9451\n",
            "Epoch 40/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0327 - accuracy: 0.9800\n",
            "Epoch 40: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.1492 - val_accuracy: 0.9451\n",
            "Epoch 41/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0249 - accuracy: 0.9800\n",
            "Epoch 41: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.1516 - val_accuracy: 0.9451\n",
            "Epoch 42/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0148 - accuracy: 0.9973 - val_loss: 0.1493 - val_accuracy: 0.9451\n",
            "Epoch 43/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 43: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.1521 - val_accuracy: 0.9451\n",
            "Epoch 44/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0186 - accuracy: 1.0000\n",
            "Epoch 44: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9451\n",
            "Epoch 45/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 45: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9451\n",
            "Epoch 46/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 46: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9451\n",
            "Epoch 47/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 47: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9451\n",
            "Epoch 48/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0168 - accuracy: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9451\n",
            "Epoch 49/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.9451\n",
            "Epoch 50/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 50: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9451\n",
            "Epoch 51/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 51: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9451\n",
            "Epoch 52/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9451\n",
            "Epoch 53/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 53: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.9451\n",
            "Epoch 54/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 54: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1677 - val_accuracy: 0.9451\n",
            "Epoch 55/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 55: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9451\n",
            "Epoch 56/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1671 - val_accuracy: 0.9451\n",
            "Epoch 57/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 57: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9451\n",
            "Epoch 58/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 58: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9451\n",
            "Epoch 59/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 59: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1715 - val_accuracy: 0.9451\n",
            "Epoch 60/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 60: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1720 - val_accuracy: 0.9451\n",
            "Epoch 61/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1733 - val_accuracy: 0.9451\n",
            "Epoch 62/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 62: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9451\n",
            "Epoch 63/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1797 - val_accuracy: 0.9451\n",
            "Epoch 64/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 64: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9451\n",
            "Epoch 65/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 65: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9451\n",
            "Epoch 66/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 66: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1904 - val_accuracy: 0.9451\n",
            "Epoch 67/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 0.9451\n",
            "Epoch 68/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 68: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1841 - val_accuracy: 0.9451\n",
            "Epoch 69/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9451\n",
            "Epoch 70/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1829 - val_accuracy: 0.9451\n",
            "Epoch 71/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 71: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1811 - val_accuracy: 0.9451\n",
            "Epoch 72/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 72: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9451\n",
            "Epoch 73/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9451\n",
            "Epoch 74/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1867 - val_accuracy: 0.9451\n",
            "Epoch 75/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 75: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1835 - val_accuracy: 0.9451\n",
            "Epoch 76/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1834 - val_accuracy: 0.9451\n",
            "Epoch 77/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 77: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9451\n",
            "Epoch 78/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9451\n",
            "Epoch 79/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9451\n",
            "Epoch 80/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9451\n",
            "Epoch 81/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 81: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1900 - val_accuracy: 0.9451\n",
            "Epoch 82/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 82: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1896 - val_accuracy: 0.9451\n",
            "Epoch 83/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 83: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9451\n",
            "Epoch 84/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9451\n",
            "Epoch 85/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1947 - val_accuracy: 0.9451\n",
            "Epoch 86/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 86: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9451\n",
            "Epoch 87/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 6.4857e-04 - accuracy: 1.0000\n",
            "Epoch 87: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1938 - val_accuracy: 0.9451\n",
            "Epoch 88/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.2348e-04 - accuracy: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9451\n",
            "Epoch 89/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1970 - val_accuracy: 0.9451\n",
            "Epoch 90/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1964 - val_accuracy: 0.9451\n",
            "Epoch 91/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 91: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9451\n",
            "Epoch 92/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.0782e-04 - accuracy: 1.0000\n",
            "Epoch 92: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1968 - val_accuracy: 0.9451\n",
            "Epoch 93/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9451\n",
            "Epoch 94/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 94: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1987 - val_accuracy: 0.9451\n",
            "Epoch 95/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 5.9613e-04 - accuracy: 1.0000\n",
            "Epoch 95: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1976 - val_accuracy: 0.9451\n",
            "Epoch 96/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 96: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9451\n",
            "Epoch 97/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 5.8860e-04 - accuracy: 1.0000\n",
            "Epoch 97: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2029 - val_accuracy: 0.9451\n",
            "Epoch 98/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 98: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2026 - val_accuracy: 0.9451\n",
            "Epoch 99/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 99: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9451\n",
            "Epoch 100/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 7.8558e-04 - accuracy: 1.0000\n",
            "Epoch 100: val_loss did not improve from 0.11560\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 베스트 모델 불러오기"
      ],
      "metadata": {
        "id": "CFE8gePfGoP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model(modelpath)"
      ],
      "metadata": {
        "id": "8-c_IYpCGs7c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 평가"
      ],
      "metadata": {
        "id": "mUEScNtX0ZUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "YY7oOuYw0J05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337f1018-770c-4453-e0f9-64e45e7bb643"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02640884928405285, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 베스트 모델 저장하고 조기 종료하기"
      ],
      "metadata": {
        "id": "bUhU2dCEzRhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout layer 추가"
      ],
      "metadata": {
        "id": "vatvDesiKBtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(100, input_dim=30, activation='relu'))\n",
        "model2.add(Dense(24, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "of6Dx8aS41gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2635b7f-5ab2-4e79-ba14-3ae79df52bfb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 100)               3100      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 24)                2424      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,549\n",
            "Trainable params: 5,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xvFK9UQ_5Okq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 조기 종료 조건 설정"
      ],
      "metadata": {
        "id": "btbD3xIVHuk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=30)"
      ],
      "metadata": {
        "id": "7bLKSzOsH06X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath2 = 'best_model2.h5'\n",
        "mc2 = ModelCheckpoint(filepath=modelpath2, monitor='val_loss', verbose=1,\n",
        "                      save_best_only=True)"
      ],
      "metadata": {
        "id": "tygK6GoyIsVp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model2.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=50,\n",
        "                      verbose=1, callbacks=[mc2, es])"
      ],
      "metadata": {
        "id": "izbsW1xN5URX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f1384b-f6ac-4a96-cdc5-74bc273f85c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/8 [==>...........................] - ETA: 3s - loss: 0.8957 - accuracy: 0.2800\n",
            "Epoch 1: val_loss improved from inf to 0.49368, saving model to best_model2.h5\n",
            "8/8 [==============================] - 1s 53ms/step - loss: 0.6922 - accuracy: 0.5357 - val_loss: 0.4937 - val_accuracy: 0.8462\n",
            "Epoch 2/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.5519 - accuracy: 0.7400\n",
            "Epoch 2: val_loss improved from 0.49368 to 0.34977, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4594 - accuracy: 0.8242 - val_loss: 0.3498 - val_accuracy: 0.9341\n",
            "Epoch 3/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2865 - accuracy: 0.9600\n",
            "Epoch 3: val_loss improved from 0.34977 to 0.26767, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3359 - accuracy: 0.9093 - val_loss: 0.2677 - val_accuracy: 0.9341\n",
            "Epoch 4/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3298 - accuracy: 0.9200\n",
            "Epoch 4: val_loss improved from 0.26767 to 0.21796, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2552 - accuracy: 0.9451 - val_loss: 0.2180 - val_accuracy: 0.9341\n",
            "Epoch 5/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2758 - accuracy: 0.8800\n",
            "Epoch 5: val_loss improved from 0.21796 to 0.18448, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2253 - accuracy: 0.9396 - val_loss: 0.1845 - val_accuracy: 0.9341\n",
            "Epoch 6/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.2246 - accuracy: 0.9400\n",
            "Epoch 6: val_loss improved from 0.18448 to 0.16220, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1881 - accuracy: 0.9478 - val_loss: 0.1622 - val_accuracy: 0.9341\n",
            "Epoch 7/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1418 - accuracy: 0.9800\n",
            "Epoch 7: val_loss improved from 0.16220 to 0.14708, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1666 - accuracy: 0.9505 - val_loss: 0.1471 - val_accuracy: 0.9451\n",
            "Epoch 8/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1738 - accuracy: 0.9400\n",
            "Epoch 8: val_loss improved from 0.14708 to 0.13833, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1468 - accuracy: 0.9643 - val_loss: 0.1383 - val_accuracy: 0.9560\n",
            "Epoch 9/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1242 - accuracy: 0.9800\n",
            "Epoch 9: val_loss improved from 0.13833 to 0.13140, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1435 - accuracy: 0.9725 - val_loss: 0.1314 - val_accuracy: 0.9451\n",
            "Epoch 10/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1269 - accuracy: 0.9600\n",
            "Epoch 10: val_loss improved from 0.13140 to 0.12795, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1206 - accuracy: 0.9753 - val_loss: 0.1280 - val_accuracy: 0.9451\n",
            "Epoch 11/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1340 - accuracy: 0.9600\n",
            "Epoch 11: val_loss improved from 0.12795 to 0.12609, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.1137 - accuracy: 0.9670 - val_loss: 0.1261 - val_accuracy: 0.9451\n",
            "Epoch 12/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1057 - accuracy: 0.9600\n",
            "Epoch 12: val_loss improved from 0.12609 to 0.12459, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0903 - accuracy: 0.9753 - val_loss: 0.1246 - val_accuracy: 0.9451\n",
            "Epoch 13/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1112 - accuracy: 0.9600\n",
            "Epoch 13: val_loss improved from 0.12459 to 0.12327, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0844 - accuracy: 0.9780 - val_loss: 0.1233 - val_accuracy: 0.9560\n",
            "Epoch 14/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0459 - accuracy: 1.0000\n",
            "Epoch 14: val_loss improved from 0.12327 to 0.12293, saving model to best_model2.h5\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0865 - accuracy: 0.9808 - val_loss: 0.1229 - val_accuracy: 0.9560\n",
            "Epoch 15/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1188 - accuracy: 0.9800\n",
            "Epoch 15: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9808 - val_loss: 0.1235 - val_accuracy: 0.9560\n",
            "Epoch 16/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0373 - accuracy: 1.0000\n",
            "Epoch 16: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0826 - accuracy: 0.9808 - val_loss: 0.1271 - val_accuracy: 0.9560\n",
            "Epoch 17/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1364 - accuracy: 0.9200\n",
            "Epoch 17: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9670 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 18/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0629 - accuracy: 0.9800\n",
            "Epoch 18: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0680 - accuracy: 0.9808 - val_loss: 0.1311 - val_accuracy: 0.9560\n",
            "Epoch 19/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0624 - accuracy: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0666 - accuracy: 0.9863 - val_loss: 0.1297 - val_accuracy: 0.9560\n",
            "Epoch 20/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1353 - accuracy: 0.9400\n",
            "Epoch 20: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0779 - accuracy: 0.9780 - val_loss: 0.1278 - val_accuracy: 0.9560\n",
            "Epoch 21/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0416 - accuracy: 0.9800\n",
            "Epoch 21: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0677 - accuracy: 0.9863 - val_loss: 0.1291 - val_accuracy: 0.9560\n",
            "Epoch 22/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 22: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9808 - val_loss: 0.1300 - val_accuracy: 0.9560\n",
            "Epoch 23/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.9890 - val_loss: 0.1307 - val_accuracy: 0.9560\n",
            "Epoch 24/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9800\n",
            "Epoch 24: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0655 - accuracy: 0.9780 - val_loss: 0.1310 - val_accuracy: 0.9560\n",
            "Epoch 25/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.1683 - accuracy: 0.9800\n",
            "Epoch 25: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0620 - accuracy: 0.9835 - val_loss: 0.1297 - val_accuracy: 0.9560\n",
            "Epoch 26/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0512 - accuracy: 0.9890 - val_loss: 0.1299 - val_accuracy: 0.9451\n",
            "Epoch 27/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9808 - val_loss: 0.1314 - val_accuracy: 0.9451\n",
            "Epoch 28/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9890 - val_loss: 0.1347 - val_accuracy: 0.9451\n",
            "Epoch 29/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 0.9863 - val_loss: 0.1365 - val_accuracy: 0.9451\n",
            "Epoch 30/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.9835 - val_loss: 0.1378 - val_accuracy: 0.9451\n",
            "Epoch 31/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 31: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9890 - val_loss: 0.1375 - val_accuracy: 0.9451\n",
            "Epoch 32/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9800\n",
            "Epoch 32: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0495 - accuracy: 0.9863 - val_loss: 0.1386 - val_accuracy: 0.9560\n",
            "Epoch 33/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9800\n",
            "Epoch 33: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.1391 - val_accuracy: 0.9560\n",
            "Epoch 34/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.9600\n",
            "Epoch 34: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0504 - accuracy: 0.9863 - val_loss: 0.1427 - val_accuracy: 0.9560\n",
            "Epoch 35/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0385 - accuracy: 0.9890 - val_loss: 0.1443 - val_accuracy: 0.9560\n",
            "Epoch 36/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9835 - val_loss: 0.1464 - val_accuracy: 0.9560\n",
            "Epoch 37/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0414 - accuracy: 0.9918 - val_loss: 0.1490 - val_accuracy: 0.9560\n",
            "Epoch 38/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9800\n",
            "Epoch 38: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9918 - val_loss: 0.1539 - val_accuracy: 0.9560\n",
            "Epoch 39/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9800\n",
            "Epoch 39: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0382 - accuracy: 0.9945 - val_loss: 0.1581 - val_accuracy: 0.9560\n",
            "Epoch 40/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 0.9945 - val_loss: 0.1632 - val_accuracy: 0.9451\n",
            "Epoch 41/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.9863 - val_loss: 0.1628 - val_accuracy: 0.9560\n",
            "Epoch 42/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9800\n",
            "Epoch 42: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0290 - accuracy: 0.9890 - val_loss: 0.1631 - val_accuracy: 0.9560\n",
            "Epoch 43/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 43: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.1650 - val_accuracy: 0.9560\n",
            "Epoch 44/100\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9800\n",
            "Epoch 44: val_loss did not improve from 0.12293\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0283 - accuracy: 0.9918 - val_loss: 0.1653 - val_accuracy: 0.9560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = load_model(modelpath2)\n",
        "best_model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "Pt2E7L3w5bJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17e7871-5ac5-4020-e3c0-972439f33b43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.028080349788069725, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_acc2 = history2.history['accuracy']\n",
        "y_vloss2 = history2.history['val_loss']\n",
        "xs2 = np.arange(1,len(y_acc2)+1)        # epoch"
      ],
      "metadata": {
        "id": "y8mGdjUD5kF_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(xs2, y_acc2, label='train accuracy')\n",
        "plt.plot(xs2, y_vloss2, label='validation loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.ylim([0,1.1])\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Best model + Early stop')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OOuC2EJB5vF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "90579ac0-5aaa-489a-fff6-b46aa2d39696"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kIWH9oEx0cMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}